{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "3bc332b8e9960f8552951caf18ea2beeda2c79e4c45f83cb055b51c376558fbc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "import numpy as np\n",
    "import torch"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 2,
   "outputs": []
  },
  {
   "source": [
    "+ 数据类型及转换"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "torch.tensor([1, 2, 3, 4])\n",
    "torch.tensor([1, 2, 3, 4]).dtype\n",
    "torch.tensor([1, 2, 3, 4], dtype=torch.double).dtype\n",
    "torch.tensor(range(10))\n",
    "np.array([1., 2., 3., 4.]).dtype\n",
    "np.array([1, 2, 3, 4]).dtype\n",
    "torch.randn((3, 3)).int()\n",
    "torch.randn([3, 3]).to(torch.int)\n",
    "torch.randn(3,3)\n",
    "torch.randint(0, 5, [3, 3]).to(torch.float)\n",
    "torch.randint(24,567, (2, 2)).double()  #float32(), float64() isn't work"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "+ 生成张量"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[5, 2, 2, 4, 5],\n",
       "         [6, 0, 8, 3, 2],\n",
       "         [3, 0, 1, 1, 4],\n",
       "         [7, 7, 1, 7, 7],\n",
       "         [8, 8, 6, 8, 6]]])"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "# 0,1均匀分布tensor\n",
    "torch.rand((3,3))\n",
    "\n",
    "# 标准正态分布tensor\n",
    "torch.randn((2, 3, 4))\n",
    "\n",
    "# 0 tensor\n",
    "torch.zeros((2, 3, 4))\n",
    "\n",
    "# 1 tensor\n",
    "torch.ones([1, 2, 3, 4])\n",
    "\n",
    "# Identity Matrix\n",
    "torch.eye(5)\n",
    "\n",
    "# 均匀分布(整数)的tensor\n",
    "torch.randint(0, 10, (1, 5, 5))"
   ]
  },
  {
   "source": [
    "+ tensor的储存设备"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Trying to store a tensor in \"cuda:1\", but: \n CUDA error: invalid device ordinal\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-1.1600,  0.5308,  0.2006],\n",
       "        [-0.6899,  0.0626,  2.4125],\n",
       "        [-0.1483,  0.1320, -0.7713]], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "# 储存在cpu上的tensor\n",
    "torch.randn((3, 3), device='cpu')\n",
    "\n",
    "# 储存在0号GPU上的一个tensor \n",
    "torch.randn((3, 3), device='cuda:0')\n",
    "\n",
    "# 储存在1号GPU上面的tensor\n",
    "try:\n",
    "    torch.randn((3, 3), device='cuda:1')  # 由于只有1块GPU，此行代码会报错，RuntimeError: CUDA error: invalid device ordinal\n",
    "except Exception as e:\n",
    "    print('Trying to store a tensor in \"cuda:1\", but: \\n', e)\n",
    "\n",
    "# 查看当前tensor所储存的设备\n",
    "torch.randn((3, 3), device='cuda:0').device\n",
    "\n",
    "# 将 GPU tensor 转移到 cpu 上\n",
    "torch.randn((3, 3), device='cuda:0').cpu()\n",
    "\n",
    "# 将 cpu tensor 转移到 GPU 上\n",
    "torch.randn((3, 3), device='cpu').cuda(0)"
   ]
  },
  {
   "source": [
    "+ tensor.view 改变形状"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0.9929, 0.5345, 1.4711, 0.1984, 1.8960, 0.2423, 1.5989, 2.2179, 0.9319,\n",
       "        0.3877, 0.0931, 0.6937, 1.0144, 1.5194])"
      ]
     },
     "metadata": {},
     "execution_count": 108
    }
   ],
   "source": [
    "t = torch.randn(12)\n",
    "t1 = torch.randint(0, 5, (2, 6))\n",
    "\n",
    "# reshape_as 会改变储存地址\n",
    "t.reshape_as(t1).data_ptr()\n",
    "\n",
    "# view，不会改变数据地址\n",
    "t.view((3, 4))\n",
    "t.data_ptr() == t.view((3, 4)).data_ptr()  # True\n",
    "\n",
    "# 改变viewed的tensor也会改变之前的tensor\n",
    "t.view((3,4))[0, 0] = 1\n",
    "t.view(3, 4).transpose(0,1)  # transpose可以选择两个轴，并转置不会影响tensor储存地址 (permute多轴转置)\n",
    "t.view(3, 4).transpose(0,1).data_ptr() == t.data_ptr()  # True\n",
    "\n",
    "# mask\n",
    "t = torch.randn((2, 3, 5))\n",
    "t > 0  # Boolean Matrix\n",
    "(t > 0).to(torch.int)\n",
    "t[t > 0]  #返回一个向量"
   ]
  },
  {
   "source": [
    "+ tensor 运算"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.0719, 0.9478, 0.0220, 0.0564],\n        [0.4653, 0.9338, 0.1548, 0.2988],\n        [0.1144, 0.2024, 0.4471, 0.8277]])\ntensor([[0.0719, 0.9478, 0.0220, 0.0564],\n        [0.4653, 0.9338, 0.1548, 0.2988],\n        [0.1144, 0.2024, 0.4471, 0.8277]])\ntensor([[0.2681, 0.9736, 0.1483, 0.2375],\n        [0.6821, 0.9663, 0.3935, 0.5467],\n        [0.3382, 0.4499, 0.6687, 0.9098]])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1.2884, 2.3898, 1.2104, 1.6940]])"
      ]
     },
     "metadata": {},
     "execution_count": 132
    }
   ],
   "source": [
    "t1 = torch.rand(3, 4)\n",
    "print(t1)\n",
    "\n",
    "# sqrt\n",
    "t1.sqrt()\n",
    "torch.sqrt(t1) == t1.sqrt()  # All True, 不会改变t1的值\n",
    "print(t1)\n",
    "\n",
    "# sqrt_() In_Place，sqrt直接作用于t1，改变t1的值\n",
    "t1. sqrt_()\n",
    "print(t1)\n",
    "\n",
    "# tensor 求和/求平均\n",
    "torch.sum(t1)  # 默认维度collapse成1维\n",
    "torch.sum(t1, 0)   # 按第0维求和\n",
    "torch.sum(t1, [0, 1])\n",
    "\n",
    "torch.mean(t1)\n",
    "torch.mean(t1, [0, 1]) == torch.mean(t1)\n",
    "\n",
    "torch.sum(t1, 0, keepdim=True)  # keepdim 需要告知operator操作维度，减少多个维度时不能够keepdim"
   ]
  },
  {
   "source": [
    "向量的4则运算"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "metadata": {},
     "execution_count": 144
    }
   ],
   "source": [
    "t1 = torch.rand(2, 3)\n",
    "t2 = torch.rand(2, 3)\n",
    "\n",
    "t1.add(t2) == torch.add(t1, t2)  # torch.add\n",
    "\n",
    "t1.sub(t2) == t1 - t2  # True\n",
    "\n",
    "t1 * t2 == t1.mul(t2)\n",
    "\n",
    "t1 / t2 == t1.div(t2)\n",
    "\n",
    "## 矩阵乘法利用 torch.matmul【适用于高维】， torch.mm【只适用于2维】\n",
    "## 同理，也存在 tensor1.add_(tensor2) 等tensor内置方法。将会直接改变tensor1的值"
   ]
  },
  {
   "source": [
    "+ 极值与排序函数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-2.0751,  1.8493, -0.7844,  0.6083],\n        [-0.8222, -1.5446,  0.3813,  0.9378],\n        [ 0.0061,  1.0957,  0.7208,  0.8012]])\ntorch.return_types.sort(\nvalues=tensor([[-2.0751, -0.7844,  0.6083,  1.8493],\n        [-1.5446, -0.8222,  0.3813,  0.9378],\n        [ 0.0061,  0.7208,  0.8012,  1.0957]]),\nindices=tensor([[0, 2, 3, 1],\n        [1, 0, 2, 3],\n        [0, 2, 3, 1]]))\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(3, 4)\n",
    "torch.argmax(t, 0)  # 第0维度的极大值位置\n",
    "t.argmin(1)\n",
    "\n",
    "torch.max(t, -1)  # 返回最后一个维度上最大值，最大值index\n",
    "torch.min(t, -1)\n",
    "\n",
    "print(t)\n",
    "print(t.sort(-1))  # 每行从小到大排序\n"
   ]
  },
  {
   "source": [
    "+ 矩阵乘法，缩并"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[True, True, True],\n",
       "         [True, True, True],\n",
       "         [True, True, True]],\n",
       "\n",
       "        [[True, True, True],\n",
       "         [True, True, True],\n",
       "         [True, True, True]]])"
      ]
     },
     "metadata": {},
     "execution_count": 176
    }
   ],
   "source": [
    "a = torch.randn(2, 3, 4)\n",
    "b = torch.randn(2, 4, 3)\n",
    "\n",
    "# batch matrix multiplication\n",
    "torch.bmm(a,b)  # 只能 3-D tensor\n",
    "a @ b == torch.bmm(a, b)  # @ 可以作为三维以下的矩阵乘法的符号\n",
    "\n",
    "a1 = torch.randn(2, 3, 4, 5)\n",
    "b1 = torch.randn(2, 6, 4, 7)\n",
    "torch.matmul(a, b)  # high_dim tensor matmul, confuse about it operate rule\n",
    "\n",
    "# **自定义高维乘法规则\n",
    "torch.einsum('bnk, bkl -> bnl', a, b) == torch.bmm(a, b)\n",
    "# ⬆ 'bnk, bkl, bnl'代表了tensor维度,字母是什么无所谓，但是要对应。\n",
    "# ⬆ k就是相加的方向， k在tensor中的维度值要相同"
   ]
  },
  {
   "source": [
    "+ tensor拼接，堆叠"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[[0, 0],\n",
       "          [1, 1],\n",
       "          [2, 2]],\n",
       " \n",
       "         [[2, 0],\n",
       "          [0, 1],\n",
       "          [2, 2]]]),\n",
       " tensor([[[1, 2],\n",
       "          [2, 2],\n",
       "          [0, 0]],\n",
       " \n",
       "         [[0, 1],\n",
       "          [2, 1],\n",
       "          [2, 0]]]))"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "t1 = torch.randn(3, 4)\n",
    "t2 = torch.randn(3, 4)\n",
    "t3 = torch.randn(3, 4)\n",
    "t4 = torch.randn(3, 2)\n",
    "\n",
    "# 沿着最后一个维度堆叠\n",
    "torch.stack((t1, t2, t3), -1).shape  # return: torch.Size([3, 4, 3])\n",
    "\n",
    "# 在最后一个维度拼接\n",
    "torch.cat([t1, t2, t3, t4], -1).shape  # return: torch.Size([3, 14])\n",
    "\n",
    "# 在最后一个维度进行分割tensor\n",
    "t = torch.randint(0, 3, [2, 3, 4])\n",
    "torch.split(t, 2, -1)  # param: Tensor, split_size_seciton, dim; split可以为一个list,分成不等长的section\n",
    "\n",
    "torch.chunk(t, 3, -1)  # param：Tensor, num, dim; num表示等分成几部分 "
   ]
  }
 ]
}