{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "3bc332b8e9960f8552951caf18ea2beeda2c79e4c45f83cb055b51c376558fbc"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "+ 朴素线性回归类\n",
    "1. 导入torch.nn库\n",
    "2. 继承nn.Module类\n",
    "3. forward 运算\n",
    "\n",
    "$120 * 92$  \n",
    "$cm*cm$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-1.9276],\n",
       "        [-1.7968],\n",
       "        [-0.1114],\n",
       "        [ 0.5583]], grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, ndim):\n",
    "        super().__init__()\n",
    "        self.ndim = ndim\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(ndim, 1))  # Weight\n",
    "        self.bias = nn.Parameter(torch.randn(1))  # Bias\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.mm(self.weight) + self.bias  # x @ W + b\n",
    "\n",
    "\n",
    "lm = LinearModel(5)  # 定义线性回归模型， 特征数5\n",
    "x = torch.randn(4, 5)\n",
    "lm(x)"
   ]
  },
  {
   "source": [
    "+ 模块方法调用"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 获取模型参数(带名字)的生成器\n",
    "lm.named_parameters()\n",
    "print(list(lm.named_parameters()), '\\n')\n",
    "\n",
    "# 获取模型参数(不带名字)的生成器\n",
    "lm.parameters()\n",
    "print(list(lm.parameters()))\n",
    "\n",
    "# 将模型参数转移到GPU上(我的vscode默认放在了GPU上)\n",
    "lm.cuda()\n",
    "print(list(lm.parameters()))\n",
    "\n",
    "# 转换模型参数为半/双精度浮点数\n",
    "lm.half()\n",
    "print(list(lm.parameters()))\n",
    "\n",
    "lm.double()\n",
    "print(list(lm.named_parameters()))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('weight', Parameter containing:\ntensor([[ 0.9766],\n        [ 0.0713],\n        [-0.0754],\n        [-0.3015],\n        [-0.4956]], device='cuda:0', dtype=torch.float64, requires_grad=True)), ('bias', Parameter containing:\ntensor([-0.4626], device='cuda:0', dtype=torch.float64, requires_grad=True))] \n\n[Parameter containing:\ntensor([[ 0.9766],\n        [ 0.0713],\n        [-0.0754],\n        [-0.3015],\n        [-0.4956]], device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\ntensor([-0.4626], device='cuda:0', dtype=torch.float64, requires_grad=True)]\n[Parameter containing:\ntensor([[ 0.9766],\n        [ 0.0713],\n        [-0.0754],\n        [-0.3015],\n        [-0.4956]], device='cuda:0', dtype=torch.float64, requires_grad=True), Parameter containing:\ntensor([-0.4626], device='cuda:0', dtype=torch.float64, requires_grad=True)]\n[Parameter containing:\ntensor([[ 0.9766],\n        [ 0.0713],\n        [-0.0754],\n        [-0.3015],\n        [-0.4956]], device='cuda:0', dtype=torch.float16, requires_grad=True), Parameter containing:\ntensor([-0.4626], device='cuda:0', dtype=torch.float16, requires_grad=True)]\n[('weight', Parameter containing:\ntensor([[ 0.9766],\n        [ 0.0713],\n        [-0.0754],\n        [-0.3015],\n        [-0.4956]], device='cuda:0', dtype=torch.float64, requires_grad=True)), ('bias', Parameter containing:\ntensor([-0.4626], device='cuda:0', dtype=torch.float64, requires_grad=True))]\n"
     ]
    }
   ]
  },
  {
   "source": [
    "+ 自动求导机制"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[-1.5095,  0.1374, -0.8820],\n        [ 0.9715, -0.0199,  0.2490],\n        [ 0.5697,  0.4074,  0.5472]], requires_grad=True)\ntensor(4.8715, grad_fn=<SumBackward0>)\ntensor([[-3.0189,  0.2749, -1.7639],\n        [ 1.9430, -0.0398,  0.4980],\n        [ 1.1394,  0.8147,  1.0944]])\ntensor([[-6.0379,  0.5497, -3.5278],\n        [ 3.8861, -0.0796,  0.9959],\n        [ 2.2788,  1.6294,  2.1889]])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "t1 = torch.randn(3, 3, requires_grad=True)\n",
    "print(t1)\n",
    "t2 = t1.pow(2).sum()\n",
    "print(t2)\n",
    "\n",
    "t2.backward()\n",
    "print(t1.grad)  # t2对t1求导，这样就可以理解了就是Andrew所提到的d_t1 = t2对t1求偏导\n",
    "\n",
    "# 梯度积累，此处的t2定义是必要的\n",
    "t2 = t1.pow(2).sum()\n",
    "t2.backward()\n",
    "print(t1.grad)\n",
    "\n",
    "# 单个tensor清零梯度\n",
    "t1.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([[-3.0189,  0.2749, -1.7639],\n",
       "         [ 1.9430, -0.0398,  0.4980],\n",
       "         [ 1.1394,  0.8147,  1.0944]]),)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "t2 = t1.pow(2).sum()\n",
    "torch.autograd.grad(t2, t1)  # derivative t2 with t1"
   ]
  },
  {
   "source": [
    "+ 空值计算图的方法示例"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t2:  tensor(4.3507, grad_fn=<SumBackward0>)\nt3:  tensor(4.3507)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(4.3507)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "t1 = torch.randn(3, 3, requires_grad=True)\n",
    "t2 = t1.sum()\n",
    "print('t2: ', t2)  # t2的计算已经构建了计算图，输出结果带有grad_fn\n",
    "\n",
    "with torch.no_grad():\n",
    "    t3 = t1.sum()\n",
    "print('t3: ', t3)  # t3d的计算没有构建计算图，输出结果并没有grad_fn\n",
    "\n",
    "t1.sum()  # 保持计算图\n",
    "t1.sum().detach()  # 和原来的计算图分离"
   ]
  },
  {
   "source": [
    "+ 损失函数以及优化器"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "t1:  tensor([-0.5820,  1.4371,  1.6877, -0.1407, -0.4090], requires_grad=True) \n t1s:  tensor([0.3585, 0.8080, 0.8439, 0.4649, 0.3992], grad_fn=<SigmoidBackward>)\nbce:  tensor(0.6797, grad_fn=<BinaryCrossEntropyBackward>)\nbce_logit:  tensor(0.6797, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "mse = nn.MSELoss()  # 初始化平方损失函数模板\n",
    "t1 = torch.randn(5, requires_grad=True)\n",
    "t2 = torch.randn(5, requires_grad=True)\n",
    "mse(t1, t2)  # 计算t1&t2的MSE\n",
    "\n",
    "t1 = torch.randn(5, requires_grad=True)\n",
    "t1s = torch.sigmoid(t1)  # sigmoid=1/(1+exp(-x))\n",
    "print('t1: ', t1, '\\n', 't1s: ', t1s)\n",
    "\n",
    "t2 = torch.randint(0, 2, (5, )).float()  # 浮点整数向量\n",
    "bce = nn.BCELoss()  # Binary Cross Entrophy Loss，接受2个参数，第一个参数是正标签的概率(sigmoided prob)，\n",
    "print('bce: ', bce(t1s, t2))  # 此处是 t1_sigmoid & t2 的计算 \n",
    "\n",
    "bce_logits = nn.BCEWithLogitsLoss()  # 交叉熵对数损失函数，先对t1求sigmoid再进行计算\n",
    "print('bce_logit: ', bce_logits(t1, t2))  # 此处是 t1 & t2 的计算\n",
    "\n",
    "N = 10 # 多分类问题 类别数目\n",
    "t1 = torch.randn(5, N, requires_grad=True)\n",
    "t2 = torch.randint(0, N, (5, ))\n",
    "t1s = torch.nn.functional.log_softmax(t1, -1)  # log(softmax())\n",
    "\n",
    "nll = nn.NLLLoss()  # Nagetive Log Likelihood Loss\n",
    "nll(t1s, t2)  # NLLLos接受一个log(softmax())的概率值以及一个onehot编码的label，element-wise product and sum it.\n",
    "ce = nn.CrossEntropyLoss()  #定义交叉熵损失函数, 相当于对NLLLoss与log(softmax())的整合concordance\n",
    "ce(t1, t2) == nll(t1s, t2)  # input original perceptron's output and label"
   ]
  },
  {
   "source": [
    "+ Optimizer \n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'data': array([[6.3200e-03, 1.8000e+01, 2.3100e+00, ..., 1.5300e+01, 3.9690e+02,\n",
       "         4.9800e+00],\n",
       "        [2.7310e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9690e+02,\n",
       "         9.1400e+00],\n",
       "        [2.7290e-02, 0.0000e+00, 7.0700e+00, ..., 1.7800e+01, 3.9283e+02,\n",
       "         4.0300e+00],\n",
       "        ...,\n",
       "        [6.0760e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         5.6400e+00],\n",
       "        [1.0959e-01, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9345e+02,\n",
       "         6.4800e+00],\n",
       "        [4.7410e-02, 0.0000e+00, 1.1930e+01, ..., 2.1000e+01, 3.9690e+02,\n",
       "         7.8800e+00]]),\n",
       " 'target': array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n",
       "        18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n",
       "        15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n",
       "        13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n",
       "        21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n",
       "        35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n",
       "        19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n",
       "        20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n",
       "        23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n",
       "        33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n",
       "        21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n",
       "        20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n",
       "        23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n",
       "        15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n",
       "        17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n",
       "        25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n",
       "        23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n",
       "        32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n",
       "        34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n",
       "        20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n",
       "        26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n",
       "        31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n",
       "        22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n",
       "        42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n",
       "        36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n",
       "        32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n",
       "        20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n",
       "        20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n",
       "        22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n",
       "        21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n",
       "        19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n",
       "        32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n",
       "        18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n",
       "        16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n",
       "        13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n",
       "         7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n",
       "        12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n",
       "        27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n",
       "         8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n",
       "         9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n",
       "        10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n",
       "        15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n",
       "        19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n",
       "        29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n",
       "        20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n",
       "        23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9]),\n",
       " 'feature_names': array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "        'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7'),\n",
       " 'DESCR': \".. _boston_dataset:\\n\\nBoston house prices dataset\\n---------------------------\\n\\n**Data Set Characteristics:**  \\n\\n    :Number of Instances: 506 \\n\\n    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\\n\\n    :Attribute Information (in order):\\n        - CRIM     per capita crime rate by town\\n        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\\n        - INDUS    proportion of non-retail business acres per town\\n        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\\n        - NOX      nitric oxides concentration (parts per 10 million)\\n        - RM       average number of rooms per dwelling\\n        - AGE      proportion of owner-occupied units built prior to 1940\\n        - DIS      weighted distances to five Boston employment centres\\n        - RAD      index of accessibility to radial highways\\n        - TAX      full-value property-tax rate per $10,000\\n        - PTRATIO  pupil-teacher ratio by town\\n        - B        1000(Bk - 0.63)^2 where Bk is the proportion of black people by town\\n        - LSTAT    % lower status of the population\\n        - MEDV     Median value of owner-occupied homes in $1000's\\n\\n    :Missing Attribute Values: None\\n\\n    :Creator: Harrison, D. and Rubinfeld, D.L.\\n\\nThis is a copy of UCI ML housing dataset.\\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/housing/\\n\\n\\nThis dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\\n\\nThe Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\\nprices and the demand for clean air', J. Environ. Economics & Management,\\nvol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\\n...', Wiley, 1980.   N.B. Various transformations are used in the table on\\npages 244-261 of the latter.\\n\\nThe Boston house-price data has been used in many machine learning papers that address regression\\nproblems.   \\n     \\n.. topic:: References\\n\\n   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\\n   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\\n\",\n",
       " 'filename': 'D:\\\\A\\\\Dtdl\\\\Anaconda\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\boston_house_prices.csv'}"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "lm = LinearModel(13)\n",
    "criterion = nn.MSELoss()\n",
    "optim = torch.optim.SGD(lm, parameters(), lr=1e-6)  # 定义优化器"
   ]
  }
 ]
}